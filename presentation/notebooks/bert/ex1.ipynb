{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e158037e",
   "metadata": {},
   "source": [
    "# Exercise 1 - Model Training and Evaluation\n",
    "\n",
    "## BERT does not use TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4c25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a3f615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (50587, 2) Val: (6324, 2) Test: (6324, 2)\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "\n",
    "# paths\n",
    "train_csv = \"../data/train.csv\"\n",
    "val_csv   = \"../data/val.csv\"\n",
    "test_csv  = \"../data/test.csv\"\n",
    "\n",
    "# load; all of them have 2 columns: Text, Label\n",
    "train = pd.read_csv(train_csv)\n",
    "val   = pd.read_csv(val_csv)\n",
    "test  = pd.read_csv(test_csv)\n",
    "\n",
    "print(\"Train:\", train.shape, \"Val:\", val.shape, \"Test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3710619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def strip_accents_unicode(s):\n",
    "    return unicodedata.normalize('NFKD', s).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "pt_stopwords = [strip_accents_unicode(w.lower()) for w in stopwords.words('portuguese')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c821fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_bert(text, pt_stopwords):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text).lower()\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    text = \" \".join([w for w in words if w not in pt_stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a7d90",
   "metadata": {},
   "source": [
    "## b) Training models\n",
    "\n",
    "Requirements:\n",
    "- use 5-fold cross-validation\n",
    "- tune key hyperparameters systematically (e.g., regularization strength 位, tree depth)\n",
    "- document your hyperparameter search process\n",
    "\n",
    "Models to use:\n",
    "- Decision Tree\n",
    "- Gaussian Naive Bayes\n",
    "- Logistic Regression with L2 regularization\n",
    "- Logistic Regression with L1 regularization\n",
    "- Multi-Layer Perceptron (MLP) - You are free to choose your architecture but up to 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92156968",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Corrected Dataset Class\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # encodings is the output of the tokenizer\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # We extract the specific index for each key (input_ids, attention_mask)\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 2. Corrected Systematic Search Function\n",
    "def run_bert_systematic_search(X_raw, y_labels, param_grid, pt_stopwords, device):\n",
    "    os.makedirs(\"../artifacts/bert/models\", exist_ok=True)\n",
    "    # Using Portuguese BERT as discussed\n",
    "    model_name = 'neuralmind/bert-base-portuguese-cased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Cleaning: No punctuation, No Portuguese stopwords\n",
    "    X_clean = np.array([clean_for_bert(t, pt_stopwords) for t in X_raw])\n",
    "    \n",
    "    results_path = \"../artifacts/bert/models/desktop/bert_cv_results.csv\"\n",
    "    metadata_path = \"../artifacts/bert/models/desktop/bert_metadata.json\"\n",
    "    best_model_dir = \"../artifacts/bert/models/desktop/bert_best_model\"\n",
    "    \n",
    "    best_f1 = -1\n",
    "    all_cv_results = []\n",
    "\n",
    "    for lr in param_grid['learning_rate']:\n",
    "        for wd in param_grid['weight_decay']:\n",
    "            print(f\"\\n--- Testing Config: LR={lr}, Weight Decay(位)={wd} ---\")\n",
    "            fold_scores = []\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(cv.split(X_clean, y_labels)):\n",
    "                # Tokenize (returns a dictionary of tensors)\n",
    "                train_encodings = tokenizer(list(X_clean[train_idx]), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "                val_encodings = tokenizer(list(X_clean[val_idx]), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "                \n",
    "                # Create the corrected datasets\n",
    "                train_dataset = BERTDataset(train_encodings, y_labels[train_idx])\n",
    "                # Validation also needs to be a Dataset if using a standard loop or Trainer\n",
    "                val_dataset = BERTDataset(val_encodings, y_labels[val_idx])\n",
    "                val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "                # Initialize Model\n",
    "                model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "                \n",
    "                # TrainingArguments (Fixed 'eval_strategy')\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir='./temp_bert_fold',\n",
    "                    learning_rate=lr,\n",
    "                    weight_decay=wd,\n",
    "                    num_train_epochs=2,\n",
    "                    per_device_train_batch_size=16,\n",
    "                    eval_strategy=\"no\",\n",
    "                    save_strategy=\"no\",\n",
    "                    report_to=\"none\"\n",
    "                )\n",
    "                \n",
    "                trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\n",
    "                trainer.train()\n",
    "                \n",
    "                # Validation Loop\n",
    "                model.eval()\n",
    "                fold_preds = []\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        # Move batch to device\n",
    "                        b_input_ids = batch['input_ids'].to(device)\n",
    "                        b_attn_mask = batch['attention_mask'].to(device)\n",
    "                        \n",
    "                        logits = model(b_input_ids, attention_mask=b_attn_mask).logits\n",
    "                        fold_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "                \n",
    "                f1 = f1_score(y_labels[val_idx], fold_preds, average='macro')\n",
    "                fold_scores.append(f1)\n",
    "                print(f\"Fold {fold+1} F1-macro: {f1:.4f}\")\n",
    "\n",
    "            # Average score for this (LR, WD) combination\n",
    "            avg_f1 = np.mean(fold_scores)\n",
    "            all_cv_results.append({\n",
    "                'learning_rate': lr, 'weight_decay': wd,\n",
    "                'mean_f1_macro': avg_f1, 'std_f1_macro': np.std(fold_scores),\n",
    "                'run_timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "            # Save if it's the best score seen so far\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                model.save_pretrained(best_model_dir)\n",
    "                tokenizer.save_pretrained(best_model_dir)\n",
    "                metadata = {'best_cv_score': avg_f1, 'best_params': {'lr': lr, 'wd': wd}}\n",
    "                with open(metadata_path, 'w') as f:\n",
    "                    json.dump(metadata, f, indent=4)\n",
    "                print(f\"New Best Score: {avg_f1:.4f}. Model saved to {best_model_dir}\")\n",
    "\n",
    "    # Final CSV logging\n",
    "    pd.DataFrame(all_cv_results).to_csv(results_path, index=False)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "476aa99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 1.12.0\n",
      "Transformers version: 5.1.0\n",
      "Is GPU available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Is GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d333af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Config: LR=2e-05, Weight Decay(位)=0.01 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d028010173ea41bfa86b4c0896ea11d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: neuralmind/bert-base-portuguese-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "c:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5060' max='5060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5060/5060 6:38:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.326709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.241578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.211578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.196157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.174721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.118422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.121319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.124062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.112572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1-macro: 0.9428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e2769b1f6f4841b981ce8b37fda282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: neuralmind/bert-base-portuguese-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "c:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5060' max='5060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5060/5060 6:31:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.323419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.240335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.207852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.185355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.179513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.120149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.107972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.119325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.102224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1-macro: 0.9441\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247101201d044126ae555ceb627a2c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: neuralmind/bert-base-portuguese-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "c:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5060' max='5060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5060/5060 6:41:44, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.327426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.234842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.212811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.187817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.182374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.123276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.124986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.120047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.108514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.103660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1-macro: 0.9441\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae745ecbb75648e89dd567c669a364fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: neuralmind/bert-base-portuguese-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "c:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5060' max='5060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5060/5060 7:52:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.325424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.233572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.192213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.189457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.181444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.125176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.116415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.106628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.114034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.110002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1-macro: 0.9432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4014c5a28794719b401b949f0efea22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: neuralmind/bert-base-portuguese-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "c:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5060' max='5060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5060/5060 6:36:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.316724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.233247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.205514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.179050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.173032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.116007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.123529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.112598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.112468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.100115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 F1-macro: 0.9415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685d61e34b104778b148ea6218cac68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Score: 0.9431. Model saved to ../artifacts/bert/models/bert_best_model\n",
      "\n",
      "--- Testing Config: LR=2e-05, Weight Decay(位)=0.1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c198fd3dd6e64b7db3db522ccff93158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: neuralmind/bert-base-portuguese-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.decoder.weight             | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "c:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5060' max='5060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5060/5060 6:28:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.334221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.242814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.214011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.194814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.166673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.121488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.128314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.128101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.111685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.110260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m search_space = {\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m2e-5\u001b[39m, \u001b[32m5e-5\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m]}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m best_meta = \u001b[43mrun_bert_systematic_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mText\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt_stopwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mrun_bert_systematic_search\u001b[39m\u001b[34m(X_raw, y_labels, param_grid, pt_stopwords, device)\u001b[39m\n\u001b[32m     83\u001b[39m         b_input_ids = batch[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     84\u001b[39m         b_attn_mask = batch[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mb_attn_mask\u001b[49m\u001b[43m)\u001b[49m.logits\n\u001b[32m     87\u001b[39m         fold_preds.extend(torch.argmax(logits, dim=\u001b[32m1\u001b[39m).cpu().numpy())\n\u001b[32m     89\u001b[39m f1 = f1_score(y_labels[val_idx], fold_preds, average=\u001b[33m'\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\generic.py:834\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    836\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1138\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, **kwargs)\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m   1121\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1130\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m   1131\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor] | SequenceClassifierOutput:\n\u001b[32m   1132\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[33;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1135\u001b[39m \u001b[33;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[33;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1148\u001b[39m     pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1150\u001b[39m     pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\generic.py:1001\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1006\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:696\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    679\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    680\u001b[39m     input_ids=input_ids,\n\u001b[32m    681\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    684\u001b[39m     past_key_values_length=past_key_values_length,\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m attention_mask, encoder_attention_mask = \u001b[38;5;28mself\u001b[39m._create_attention_masks(\n\u001b[32m    688\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    689\u001b[39m     encoder_attention_mask=encoder_attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    693\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    694\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m sequence_output = encoder_outputs.last_hidden_state\n\u001b[32m    708\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:452\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    442\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    450\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m         hidden_states = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    456\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPastAndCrossAttentions(\n\u001b[32m    463\u001b[39m         last_hidden_state=hidden_states,\n\u001b[32m    464\u001b[39m         past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    465\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:397\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    396\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     self_attention_output, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m     attention_output = self_attention_output\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:326\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    316\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    317\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    323\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    324\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m    325\u001b[39m     attention_mask = attention_mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m encoder_attention_mask\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     attention_output, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(attention_output, hidden_states)\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_output, attn_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:182\u001b[39m, in \u001b[36mBertSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m query_layer = \u001b[38;5;28mself\u001b[39m.query(hidden_states).view(*hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    181\u001b[39m key_layer = \u001b[38;5;28mself\u001b[39m.key(hidden_states).view(*hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m value_layer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(*hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# decoder-only bert can have a simple dynamic cache for example\u001b[39;00m\n\u001b[32m    186\u001b[39m     current_past_key_values = past_key_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrador\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 3. Execution ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "search_space = {'learning_rate': [2e-5, 5e-5], 'weight_decay': [0.01, 0.1]}\n",
    "best_meta = run_bert_systematic_search(train['Text'].values, train['Label'].astype(int).values, search_space, pt_stopwords, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339f453",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ae68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (classification_report, f1_score, accuracy_score, \n",
    "                             precision_score, recall_score, roc_curve, auc)\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "322fac48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d045ed1bab44b8796aa9b9954eb6a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fa53cdfd3c4f67b2471f9d573ffd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54936236fa874dd38963b891d3bf3ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_bert_model(type):\n",
    "    # Points to the directory saved by model.save_pretrained()\n",
    "    best_model_dir = f\"../artifacts/bert/models/{type}/bert_best_model\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(best_model_dir)\n",
    "    model = BertForSequenceClassification.from_pretrained(best_model_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_bert_desktop, bert_tokenizer_desktop = load_bert_model(\"desktop\")\n",
    "best_bert_desktop.to(device)\n",
    "best_bert_desktop.eval()\n",
    "\n",
    "best_bert_kaggle_dudu, bert_tokenizer_kaggle_dudu = load_bert_model(\"kaggle-dudu\")\n",
    "best_bert_kaggle_dudu.to(device)\n",
    "best_bert_kaggle_dudu.eval()\n",
    "\n",
    "best_bert_kaggle_dani, bert_tokenizer_kaggle_dani = load_bert_model(\"kaggle-dani\")\n",
    "best_bert_kaggle_dani.to(device)\n",
    "best_bert_kaggle_dani.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2c9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_predictions(model, tokenizer, texts, device, batch_size=16):\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = list(texts[i:i+batch_size])\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                           max_length=128, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy()) # Probability of Positive Class\n",
    "            \n",
    "    return np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "# 3. Validation and Test Helpers\n",
    "def evaluate_bert_model(model, tokenizer, texts, y_true, name):\n",
    "    y_pred, _ = get_bert_predictions(model, tokenizer, texts, device)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"\\n{name}  Validation F1-macro: {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return f1\n",
    "\n",
    "def evaluate_bert_test(model, tokenizer, texts, y_true):\n",
    "    y_pred, _ = get_bert_predictions(model, tokenizer, texts, device)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision_macro\": precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"recall_macro\": recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0546325",
   "metadata": {},
   "source": [
    "## c) Comparison table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e643e75",
   "metadata": {},
   "source": [
    "### Based on test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5559e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT  Validation F1-macro: 0.9440\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      3141\n",
      "           1       0.94      0.94      0.94      3183\n",
      "\n",
      "    accuracy                           0.94      6324\n",
      "   macro avg       0.94      0.94      0.94      6324\n",
      "weighted avg       0.94      0.94      0.94      6324\n",
      "\n",
      "\n",
      "BERT  Validation F1-macro: 0.9456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      3141\n",
      "           1       0.95      0.95      0.95      3183\n",
      "\n",
      "    accuracy                           0.95      6324\n",
      "   macro avg       0.95      0.95      0.95      6324\n",
      "weighted avg       0.95      0.95      0.95      6324\n",
      "\n",
      "\n",
      "BERT  Validation F1-macro: 0.9470\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      3141\n",
      "           1       0.95      0.95      0.95      3183\n",
      "\n",
      "    accuracy                           0.95      6324\n",
      "   macro avg       0.95      0.95      0.95      6324\n",
      "weighted avg       0.95      0.95      0.95      6324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val['text_bert'] = val['Text'].apply(lambda x: clean_for_bert(x, pt_stopwords))\n",
    "test['text_bert'] = test['Text'].apply(lambda x: clean_for_bert(x, pt_stopwords))\n",
    "val_texts = val['text_bert'].values\n",
    "test_texts = test['text_bert'].values\n",
    "y_val_labels = val['Label'].astype(int).values\n",
    "y_test_labels = test['Label'].astype(int).values\n",
    "\n",
    "val_results = {\n",
    "    \"BERT-PC\": evaluate_bert_model(best_bert_desktop, bert_tokenizer_desktop, val_texts, y_val_labels, \"BERT\"),\n",
    "    \"BERT-DUDU\": evaluate_bert_model(best_bert_kaggle_dudu, bert_tokenizer_kaggle_dudu, val_texts, y_val_labels, \"BERT\"),\n",
    "    \"BERT-DANI\": evaluate_bert_model(best_bert_kaggle_dani, bert_tokenizer_kaggle_dani, val_texts, y_val_labels, \"BERT\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e2b16ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Set Comparison Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERT-PC</th>\n",
       "      <td>0.947660</td>\n",
       "      <td>0.947738</td>\n",
       "      <td>0.947675</td>\n",
       "      <td>0.947658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT-DUDU</th>\n",
       "      <td>0.946078</td>\n",
       "      <td>0.946087</td>\n",
       "      <td>0.946074</td>\n",
       "      <td>0.946078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT-DANI</th>\n",
       "      <td>0.945446</td>\n",
       "      <td>0.945450</td>\n",
       "      <td>0.945450</td>\n",
       "      <td>0.945446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy  precision_macro  recall_macro  f1_macro\n",
       "BERT-PC    0.947660         0.947738      0.947675  0.947658\n",
       "BERT-DUDU  0.946078         0.946087      0.946074  0.946078\n",
       "BERT-DANI  0.945446         0.945450      0.945450  0.945446"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results = {\n",
    "    \"BERT-PC\": evaluate_bert_test(best_bert_desktop, bert_tokenizer_desktop, test_texts, y_test_labels),\n",
    "    \"BERT-DUDU\": evaluate_bert_test(best_bert_kaggle_dudu, bert_tokenizer_kaggle_dudu, test_texts, y_test_labels),\n",
    "    \"BERT-DANI\": evaluate_bert_test(best_bert_kaggle_dani, bert_tokenizer_kaggle_dani, test_texts, y_test_labels)\n",
    "}\n",
    "\n",
    "# 5. Build Comparison Table\n",
    "df_test = pd.DataFrame(test_results).T\n",
    "print(\"\\n=== Test Set Comparison Table ===\")\n",
    "display(df_test)\n",
    "\n",
    "# Save results\n",
    "output_path = \"../artifacts/bert/models/test_results.csv\"\n",
    "df_test.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4d2456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model based on Test F1-macro: BERT-PC\n"
     ]
    }
   ],
   "source": [
    "# select best model (based on F1-macro)\n",
    "best_bert_name = df_test[\"f1_macro\"].idxmax()\n",
    "print(\"\\nBest model based on Test F1-macro:\", best_bert_name)\n",
    "\n",
    "# retrieve actual model\n",
    "best_bert = {\n",
    "    \"BERT-PC\": best_bert_desktop,\n",
    "    \"BERT-DUDU\": best_bert_kaggle_dudu,\n",
    "    \"BERT-DANI\": best_bert_kaggle_dani,\n",
    "}[best_bert_name]\n",
    "\n",
    "# retrieve actual tokenizer\n",
    "best_bert_tokenizer = {\n",
    "    \"BERT-PC\": bert_tokenizer_desktop,\n",
    "    \"BERT-DUDU\": bert_tokenizer_kaggle_dudu,\n",
    "    \"BERT-DANI\": bert_tokenizer_kaggle_dani,\n",
    "}[best_bert_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d5e15",
   "metadata": {},
   "source": [
    "### ROC Curve + Macro-Averaged AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b2c87dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final AUC: 0.9905\n"
     ]
    }
   ],
   "source": [
    "y_pred_test, y_score_test = get_bert_predictions(best_bert, best_bert_tokenizer, test_texts, device)\n",
    "fpr, tpr, _ = roc_curve(y_test_labels, y_score_test, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"BERT ROC (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], lw=2, linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve  BERTimbau (Portuguese BERT)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal AUC: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
